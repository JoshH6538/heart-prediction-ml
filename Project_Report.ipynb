{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e9d5ac-753d-49a4-bbc5-3f2ca0512c4a",
   "metadata": {},
   "source": [
    "## Heart Failure Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c384249-f544-4f26-987e-f1b1f18459e3",
   "metadata": {},
   "source": [
    "Team Members: Joshua Hanscom,  Andrew Rivera and Abigail Diaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f30501-c585-4b36-834e-c26f29ba7021",
   "metadata": {},
   "source": [
    "Course: CS4661 – Data Science / Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3f4ed-cd5d-4865-9497-440f744d731a",
   "metadata": {},
   "source": [
    "Instructor: Professor Mohammad Pourhomayoun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e897d8e-d6ad-4c78-ad50-b34086929cd7",
   "metadata": {},
   "source": [
    "Date: December 01, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279d291-a463-4664-a48a-f7b6816338ae",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Heart failure is a serious medical condition influenced by a combination of demographic, clinical, and physiological factors. Early prediction of potential heart failure can support proactive care and improve patient outcomes.\n",
    "\n",
    "In this project, we analyze the **Heart Failure Prediction** dataset to identify factors associated with heart failure and build models capable of predicting patient survival or risk of heart failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eccc73-ba16-4606-8a50-1798aab66caa",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "\n",
    "1. **Classification:** Predict whether a patient is likely to experience heart failure (or survive) based on health attributes.\n",
    "2. **Clustering:** Use unsupervised learning to identify subgroups of patients with similar risk profiles.\n",
    "3. **Feature Analysis:** Investigate which features are most strongly associated with heart failure to uncover key risk indicators.\n",
    "\n",
    "By comparing multiple modeling strategies, we aim to determine which methods provide the most accurate and meaningful insights into heart failure risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b09941-86b4-4d05-8368-4047d78e5f70",
   "metadata": {},
   "source": [
    "## 3. Dataset Description\n",
    "\n",
    "The dataset, sourced from [Kaggle - Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction), contains **918 unique patient records**, each with **11 features** and a binary target variable `HeartDisease` (1 = yes, 0 = no)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26f155-0ef1-47d7-a0c8-68c58b348389",
   "metadata": {},
   "source": [
    "**Features include:**\n",
    "\n",
    "- Age\n",
    "- Sex\n",
    "- ChestPainType\n",
    "- RestingBP\n",
    "- Cholesterol\n",
    "- FastingBS\n",
    "- RestingECG\n",
    "- MaxHR\n",
    "- ExerciseAngina\n",
    "- Oldpeak\n",
    "- ST_Slope\n",
    "\n",
    "The target variable indicates whether the patient has experienced or is at risk of heart failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa410f-ed8f-4d42-9b36-5cdc8f9cc6de",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Before model training, the dataset will be cleaned and prepared as follows:\n",
    "\n",
    "- **Initial Observation** check dataset overall shape and data types.\n",
    "- **Handle missing values** (if any)\n",
    "- **Handle duplicate records** (if any)\n",
    "- **Split data** into training and testing subsets for evaluation consistency\n",
    "- **Scale numerical variables** as required by model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59603e-55aa-4275-bcc0-ee1a0c9c4c54",
   "metadata": {},
   "source": [
    "### 4.1 Data Loading and Initial Exploration\n",
    "We loaded the Heart Failure Prediction dataset from Kaggle's CSV file \n",
    "(heart.csv) into a pandas DataFrame named df. To verify successful loading, \n",
    "we examined the DataFrame's shape, first few rows, structure, data types and missing or duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30ead38-1e7e-496a-90ee-06ed26578d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset/heart.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2b964-80b8-436b-91d9-910b5942c045",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "The dataset contains mixed data types requiring preprocessing: numerical features (Age, RestingBP, Cholesterol, MaxHR, Oldpeak) will need scaling for distance-based models, while categorical features (Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope) require one-hot encoding. A comprehensive data exploration and preprocessing pipeline is described in Section 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1360ae4-5f01-4e9b-96b6-fdd8fcf428ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (918, 12)\n",
      "Patient records: 918\n",
      "Total columns: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Patient records: {df.shape[0]}\")\n",
    "print(f\"Total columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a634cc-0e6a-475a-84f0-c9f3a12fcbfc",
   "metadata": {},
   "source": [
    "The shape `(918, 12)` establishes our dataset baseline: 918 patients and 12 \n",
    "variables. This is critical for subsequent quality checks, since we expect all columns \n",
    "to contain 918 non-null values if the data is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c23540-fd5b-4a1e-aca0-47688ac11927",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "Missing data can significantly impact model performance by introducing bias, \n",
    "reducing statistical power, or causing errors during training. Before proceeding \n",
    "with analysis, we must verify data completeness.\n",
    "\n",
    "We examined the dataset structure to identify any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8a421e4-d364-43e8-afa4-e3c218ad28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Shows column types, non-null counts, memory usage\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98418ae9-9236-4733-ae26-b9f5788002c1",
   "metadata": {},
   "source": [
    "The `df.info()` output above confirms data integrity. For clarity, we present \n",
    "a formatted summary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "091b91ec-5af4-4116-aacd-987daeb42814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure and Completeness Check:\n",
      "            Column  Non-Null Count Data Type\n",
      "0              Age             918     int64\n",
      "1              Sex             918    object\n",
      "2    ChestPainType             918    object\n",
      "3        RestingBP             918     int64\n",
      "4      Cholesterol             918     int64\n",
      "5        FastingBS             918     int64\n",
      "6       RestingECG             918    object\n",
      "7            MaxHR             918     int64\n",
      "8   ExerciseAngina             918    object\n",
      "9          Oldpeak             918   float64\n",
      "10        ST_Slope             918    object\n",
      "11    HeartDisease             918     int64\n"
     ]
    }
   ],
   "source": [
    "# Check structure and data types\n",
    "print(\"Dataset Structure and Completeness Check:\")\n",
    "# Info table\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Non-Null Count': df.count().values,\n",
    "    'Data Type': df.dtypes.values\n",
    "})\n",
    "\n",
    "print(info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620236f-5ceb-4932-abfc-e8b137cfba5d",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The **Non-Null Count** column confirms data completeness: all features show \n",
    "918 non-null values, matching our total record count from `shape`. This \n",
    "indicates no missing data, allowing us to proceed without imputation and \n",
    "preserve the full sample for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78217966-f8ed-44cd-8a87-777005dcd39f",
   "metadata": {},
   "source": [
    "### Handling Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3a8c60e-22f3-4d60-a37d-88cc9705731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicate patient records\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate records\n",
    "print(\"Checking for duplicate patient records\")\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc8cc4-694d-4521-b319-9ccf62674d51",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "No duplicate records were found in the dataset. Each of the 918 entries \n",
    "represents a unique patient record, ensuring data integrity for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76568880-35b8-4f94-936f-6df385724759",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Initial inspection of the dataset using `df.info()`revealed no missing values across all 918 records and 12 columns. Additionally, a check for duplicate entries using `df.duplicated().sum()` confirmed that each \n",
    "patient record is unique. Given the completeness and integrity of the data, no \n",
    "cleaning steps is required and we proceed directly to feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7ad70-835b-4a63-8144-cadcd920feb8",
   "metadata": {},
   "source": [
    "### 4.2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04eff4b-9efd-493c-92a7-5e9b9be81160",
   "metadata": {},
   "source": [
    "Now that we’ve verified the dataset’s integrity, we can construct the **feature matrix (`X`)** and **label vector (`y`)** for model training.\n",
    "\n",
    "We begin by selecting all relevant feature columns from the dataset.  \n",
    "\n",
    "The target variable, `HeartDisease`, will serve as our label vector (`y`), while the remaining columns form the feature matrix (`X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0101edcf-310a-44e6-86ae-e2b9ab12c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease Distribution:\n",
      "No heart disease = 410\n",
      "Heart disease = 508\n"
     ]
    }
   ],
   "source": [
    "# label vector\n",
    "y = df['HeartDisease']\n",
    "\n",
    "counts = y.value_counts().to_dict()\n",
    "\n",
    "print(\"HeartDisease Distribution:\")\n",
    "print(f\"No heart disease = {counts[0]}\")\n",
    "print(f\"Heart disease = {counts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12782bee-a555-44d3-8972-8561f000634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>65</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>130</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>115</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>TA</td>\n",
       "      <td>110</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>150</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149</td>\n",
       "      <td>N</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>110</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>65</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>136</td>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>140</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>130</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>96</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>TA</td>\n",
       "      <td>148</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>LVH</td>\n",
       "      <td>178</td>\n",
       "      <td>N</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>43</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>130</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>162</td>\n",
       "      <td>N</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>114</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>140</td>\n",
       "      <td>N</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0     40   M           ATA        140          289          0     Normal   \n",
       "100   65   M           ASY        130          275          0         ST   \n",
       "200   47   M            TA        110          249          0     Normal   \n",
       "300   60   M           ASY        160            0          1     Normal   \n",
       "400   50   F           ASY        160            0          1     Normal   \n",
       "500   65   M           ASY        136          248          0     Normal   \n",
       "600   57   M           ASY        130          207          0         ST   \n",
       "700   42   M            TA        148          244          0        LVH   \n",
       "800   43   M           NAP        130          315          0     Normal   \n",
       "900   58   M           ASY        114          318          0         ST   \n",
       "\n",
       "     MaxHR ExerciseAngina  Oldpeak ST_Slope  \n",
       "0      172              N      0.0       Up  \n",
       "100    115              Y      1.0     Flat  \n",
       "200    150              N      0.0       Up  \n",
       "300    149              N      0.4     Flat  \n",
       "400    110              N      0.0     Flat  \n",
       "500    140              Y      4.0     Down  \n",
       "600     96              Y      1.0     Flat  \n",
       "700    178              N      0.8       Up  \n",
       "800    162              N      1.9       Up  \n",
       "900    140              N      4.4     Down  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = df.columns[:-1]\n",
    "\n",
    "# feature matrix\n",
    "X = df[features_cols]\n",
    "X[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b003c-e1f1-43c8-98f3-e5e3edfe1112",
   "metadata": {},
   "source": [
    "With the data prepared, we now split it into **training** and **testing** subsets to evaluate how well the model generalizes to unseen data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce81ee-7f18-45b3-b2d6-9b2ace94b8b7",
   "metadata": {},
   "source": [
    "### Splitting the Data for Model Evaluation\n",
    "\n",
    "We explore two common evaluation strategies to assess model performance:\n",
    "\n",
    "1. **k-Fold Cross-Validation** – The training set is further evaluated using k-fold cross-validation, which repeatedly trains and tests the model across multiple folds of the data. This provides a stable estimate of model performance, helps guide hyperparameter selection, and reduces sensitivity to any particular partition of the training data.\n",
    "\n",
    "2. **Standard Train/Test Split** – The dataset is split into an 80% training set and a 20% hold-out test set. This provides a quick baseline evaluation of each model’s performance on unseen data. The test set remains untouched during training, ensuring an unbiased estimate of generalization.\n",
    "\n",
    "By combining these two strategies, we obtain both robust performance estimates via cross-validation and final, unbiased evaluation using the hold-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab83c29-ea9b-4d9b-9eda-04f34ac0b981",
   "metadata": {},
   "source": [
    "### Standard Train/Test Split\n",
    "\n",
    "We begin by establishing a **baseline performance** for our models using a simple 80/20 train–test split.  \n",
    "This approach provides an initial benchmark for accuracy and other key metrics before applying more rigorous validation methods such as k-fold cross-validation.\n",
    "\n",
    "The data is split into **training** and **testing** subsets using `train_test_split`.\n",
    "\n",
    "We will use the following parameters: `test_size`=**0.2**, `random_state`=**42**.\n",
    "\n",
    "Our *test size* indicates that our training dataset will take up 80% of the total dataset while the testing set takes up 20%.\n",
    "\n",
    "Our *random state* is a seed that allows us to have replicable results when splitting the data.\n",
    "\n",
    "We then train the Decision Tree model on the training data and evaluate it on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "947a212e-4f3c-4676-8005-bf92aa946c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcebd4-0656-45a6-bc1f-f216093224fe",
   "metadata": {},
   "source": [
    "### 4.3 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb675e2-da51-4a23-96ec-1e40d10af0f2",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3968b8-50f3-4fc0-9569-a64dec0e024f",
   "metadata": {},
   "source": [
    "Before training our models, we standardize all numerical features to ensure they are on a comparable scale. This is important because we use both distance-based models (KNN and K-Means) and gradient-based models (Logistic Regression), which can be heavily influenced by differences in feature magnitude. Without scaling, variables with larger numeric ranges (e.g., Cholesterol, MaxHR) could dominate the learning process and distort model performance.\n",
    "\n",
    "We use `StandardScaler` to transform numerical features by removing the mean and scaling to unit variance. This centers the data around zero and ensures all numeric features contribute proportionally to the model.\n",
    "\n",
    "A crucial detail is that scaling must be fit only on the training data—not the entire dataset.\n",
    "If we scale using all data before splitting or before cross-validation, the scaler “sees” information from the test set or validation folds. This results in data leakage, giving the model access to information it should not have during training. This contaminates the evaluation and leads to overly optimistic results.\n",
    "\n",
    "To avoid this, the scaler is fit on the training set only, then applied (transformed) to both the training and test sets.\n",
    "\n",
    "Decision Tree, Random Forest and Gradient Boosted Trees does not require feature scaling despite the scale differences in the dataset. This is because these models work with relative ordering and split points. Threshold values for each feature depend on information gain rather than scale, meaning the tree structure is unaffected by differences in feature magnitude.\n",
    "\n",
    "> **Note:** Categorical variables will be handled separately through one-hot encoding and are *NOT* affected by scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "312f5580-22f5-4993-bacf-21595bd05f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
      "795 -1.245067   M           NAP  -0.708985     0.372803   1.842609     Normal   \n",
      "25  -1.886236   M           NAP  -0.166285     0.086146  -0.542709     Normal   \n",
      "84   0.250993   M           ASY   0.919115     0.123134   1.842609     Normal   \n",
      "10  -1.779375   F           NAP  -0.166285     0.104640  -0.542709     Normal   \n",
      "344 -0.283314   M           ASY  -0.708985    -1.846478   1.842609     Normal   \n",
      "..        ...  ..           ...        ...          ...        ...        ...   \n",
      "106 -0.603898   F           ASY  -0.708985     0.502261  -0.542709         ST   \n",
      "270 -0.924483   M           ASY  -0.708985     0.234098  -0.542709     Normal   \n",
      "860  0.678439   M           ASY  -0.166285     0.493014  -0.542709     Normal   \n",
      "435  0.678439   M           ASY   1.027656    -1.846478  -0.542709         ST   \n",
      "102 -1.458790   F           ASY   0.919115     1.778348  -0.542709     Normal   \n",
      "\n",
      "        MaxHR ExerciseAngina   Oldpeak ST_Slope  \n",
      "795  2.284353              N -0.097061     Down  \n",
      "25   1.652241              N -0.836286       Up  \n",
      "84  -0.441628              Y  0.087745     Flat  \n",
      "10   0.229991              N -0.836286       Up  \n",
      "344 -1.271274              N -0.836286     Flat  \n",
      "..        ...            ...       ...      ...  \n",
      "106 -1.034232              N -0.836286       Up  \n",
      "270  0.150977              N -0.836286       Up  \n",
      "860  0.309005              Y  0.457358       Up  \n",
      "435 -0.718176              Y -0.836286       Up  \n",
      "102 -0.244093              N  1.011777     Flat  \n",
      "\n",
      "[734 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_feature_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_feature_cols])\n",
    "\n",
    "# Transform the training and test numeric columns\n",
    "X_train_scaled_array = scaler.transform(X_train[num_feature_cols])\n",
    "X_test_scaled_array = scaler.transform(X_test[num_feature_cols])\n",
    "\n",
    "# Rebuild DataFrames so everything stays consistent\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[num_feature_cols] = pd.DataFrame(\n",
    "    X_train_scaled_array, \n",
    "    columns=num_feature_cols, \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled[num_feature_cols] = pd.DataFrame(\n",
    "    X_test_scaled_array, \n",
    "    columns=num_feature_cols, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(X_train_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37432130-a985-491f-b015-91ce2700f421",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "\n",
    "Several features in this dataset are categorical (e.g., `Sex`, `ChestPainType`, `RestingECG`, `ExerciseAngina`, `ST_Slope`) and must be encoded before model training. Since scikit-learn's models, including Logistic Regression, KNN, Random Forests, Gradient Boosting, and Decision Trees—require numeric inputs, these values cannot be used in their raw text form.\n",
    "\n",
    "We use scikit-learn's `OneHotEncoder` to convert each categorical column into a set of binary indicator variables (0/1). This avoids incorrectly introducing an ordinal relationship between categories and allows the model to treat each category independently.\n",
    "\n",
    "**Note:** We set `sparse_output=False` so the encoder returns a dense NumPy array, which can be easily converted into a Pandas DataFrame.\n",
    "\n",
    "#### Why the Encoder Is Fit on the Training Data Only\n",
    "\n",
    "Just like scaling, `OneHotEncoder` must be fit using only the training data. This prevents **data leakage**, which happens when information from the test set unintentionally influences the training process.\n",
    "\n",
    "If the encoder is fit on the entire dataset:\n",
    "- It \"sees\" categories from the test set ahead of time\n",
    "- The test set is no longer truly unseen\n",
    "- Evaluation metrics become overly optimistic and invalid\n",
    "\n",
    "To avoid this, the correct workflow is:\n",
    "\n",
    "1. **Fit** the encoder on the training set (learns the categories)\n",
    "2. **Transform** both training and test sets using this fitted encoder\n",
    "3. During cross-validation, the encoder is fit inside each fold using only the fold's training data\n",
    "\n",
    "This ensures that at every stage, the model only has access to information available during training, maintaining fair and valid evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2587e8-a849-41e2-a3a4-2fa28ce58c96",
   "metadata": {},
   "source": [
    "   \n",
    "## 4.4 Training Models\n",
    "   [Split once here - use same split for all models for fair comparison]\n",
    "   [This ensures all models are evaluated on the same test set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb78c9-5420-4cb3-a4f6-f891788f545e",
   "metadata": {},
   "source": [
    "   \n",
    "### Model-Specific Preprocessing\n",
    "   [Explain that different models require different preprocessing]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509994ae-c6d4-46a8-b267-97249b16f988",
   "metadata": {},
   "source": [
    "## Methods / Models Used\n",
    "\n",
    "We explore both **supervised** and **unsupervised** learning methods:\n",
    "\n",
    "### Supervised Models\n",
    "\n",
    "- **Logistic Regression:** Baseline linear model to identify features most strongly influencing heart failure risk.\n",
    "- **Decision Tree:** A baseline tree-based model providing a visual, hierarchical representation of how features split and contribute to heart failure prediction.”\n",
    "- **Random Forest:** Captures non-linear relationships and provides robust feature importance metrics.\n",
    "- **Gradient Boosted Trees (GB Trees):** Improves accuracy through sequential learning and weighted updates.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "- **Clustering (e.g., K-Means):** Used to explore underlying patterns or patient subgroups within the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b920792-2f43-42b6-921f-b920be41e4d8",
   "metadata": {},
   "source": [
    "## Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ef184-937d-46b3-84e1-09926aad7a53",
   "metadata": {},
   "source": [
    "In this project, we do not use the full dataset directly for training and evaluation, as this would risk overestimating the model’s performance on unseen data. Instead, we split the dataset into a training set and a hold-out test set, reserving the test set for an unbiased evaluation of the final models. On the training set, we apply 10-fold cross-validation, which repeatedly trains and validates the models on different subsets of the training data. This provides a stable estimate of model performance, guides hyperparameter selection, and helps prevent overfitting. Importantly, cross-validation scores are not directly comparable to test set accuracy, because cross-validation only uses parts of the training data and does not reflect performance on truly unseen data. For consistency and fairness, the same cross-validation procedure is applied to all models. After selecting the best hyperparameters, each model is fitted on the full training set and evaluated once on the hold-out test set, providing an unbiased measure of generalization. Performance metrics such as accuracy, ROC curves, and AUC are reported based on both cross-validation and test set evaluation, allowing a thorough comparison of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a83cd-ecec-4611-8b1f-d87b0b431073",
   "metadata": {},
   "source": [
    "### Cross Validation -- finding hyperparameters, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec4477-7059-4aff-a86b-c1b9bc39836f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26497e1d-db99-4ff5-a1dc-e84dae9ea049",
   "metadata": {},
   "source": [
    "Model performance will be compared using metrics such as:\n",
    "\n",
    "- Accuracy\n",
    "- Precision / Recall\n",
    "- F1-Score\n",
    "- ROC-AUC\n",
    "\n",
    "Visualizations (confusion matrices, ROC curves, feature importance plots) will accompany the results.\n",
    "\n",
    "###  Model Performance Comparison\n",
    "[Table/chart comparing all models' accuracy, precision, recall, F1]\n",
    "\n",
    "### Feature Importance Analysis Across Models\n",
    "[Compare what each model considers important]\n",
    "\n",
    "### Model-Specific Insights\n",
    "   - Logistic Regression: [interpretation of coefficients]\n",
    "   - Decision Tree: [tree structure insights]\n",
    "   - Random Forest: [ensemble patterns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37361e3-88c5-4428-8593-7390bb03264c",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This section will include:\n",
    "\n",
    "- Comparison of model performance and interpretability\n",
    "- Key insights from logistic regression coefficients and tree-based feature importances\n",
    "- Observations from clustering and subgroup analysis\n",
    "- Limitations due to dataset size, imbalance, or potential bias\n",
    "  \n",
    "\n",
    "Why do models agree/disagree?\n",
    "\n",
    "\"All models agreed on ST_slope_Up, suggesting this is a robust finding\"\n",
    "\"Random Forest showed more distributed importance, likely because it captures feature interactions that single trees miss\"\n",
    "\n",
    "\n",
    "What does this mean for heart disease prediction?\n",
    "\n",
    "Clinical implications\n",
    "Which features healthcare providers should prioritize\n",
    "\n",
    "\n",
    "Limitations:\n",
    "\n",
    "\"Decision Tree may overfit to ST_slope_Up\"\n",
    "\"Feature importance doesn't reveal interactions between features\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f7cdf-1569-4f6e-a0d7-d254a07044ef",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We will summarize findings, highlight effective prediction methods, and discuss potential improvements such as:\n",
    "\n",
    "- Collecting larger or more diverse datasets\n",
    "- Incorporating additional health metrics\n",
    "- Applying deep learning or ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181eb304-45f2-42df-8d1a-415fe2cfb00a",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "### Team Contributions\n",
    "The authors contributed to this work as follows:\n",
    "\n",
    "A. Joshua Hanscom led data preprocessing, model training,\n",
    "    exploratory data analysis, performance evaluation and model development for \n",
    "    Logistic Regression and Gradient Boosted Trees (GB Trees).\n",
    "\n",
    "B. Andrew Rivera contributed to model training and performance evaluation.\n",
    "\n",
    "C. Abigail Diaz handled feature engineering, visualization, and report writing.\n",
    "\n",
    "All authors reviewed and approved the final manuscript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
